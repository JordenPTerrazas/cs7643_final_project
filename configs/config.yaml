Train:
  batch_size: 16
  lr: 0.01
  betas: [0.9, 0.999]
  weight_decay: 0.0034
  epochs: 5
  anneal_epochs: 3
  gamma: 0.3
 # Remove sigmoid from the gating in GLFBs to counteract the vanishing gradient, brought LR back down
 # and chance anneal epochs so we bump learning rate up later in training, bringing gamma back to normal

network:
  model: MFNetNoSigmoid
  load_checkpoint: False
  checkpoint_model_path: blah
  checkpoint_optimizer_path: blah
  checkpoint_scheduler_path: blah
  start_epoch: 0

data:
  train_data_dir: data/datasets/DNS_subset_10/train/noisy
  train_label_dir: data/datasets/DNS_subset_10/train/clean
  val_data_dir: data/datasets/DNS_subset_10/val/noisy
  val_label_dir: data/datasets/DNS_subset_10/val/clean
  save_every: 900
  save_dir: models/run_5
  save_best: True